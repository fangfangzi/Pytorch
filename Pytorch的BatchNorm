import torch
from torch import nn
from torch.nn import init
import torch.utils.data as Data
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import torch.autograd.variable as Variable


N_SAMPLES = 2000
BATCH_SIZE = 64
EPOCH = 12
LR = 0.03
N_HIDDEN = 8
ACTIVATION = F.tanh
B_INIT = -0.2

#训练数据
x = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]
noise = np.random.normal(0, 2, x.shape)
y = np.square(x) - 5 + noise

# test data
test_x = np.linspace(-7, 10, 200)[:, np.newaxis]
noise = np.random.normal(0, 2, test_x.shape)
test_y = np.square(test_x) - 5 + noise

#Tensor操作
train_x, train_y = torch.from_numpy(x).float(), torch.from_numpy(y).float()
test_x = torch.from_numpy(test_x).float()
test_y = torch.from_numpy(test_y).float()

train_dataset = Data.TensorDataset(train_x, train_y)
train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.norm = nn.BatchNorm1d(1,eps=1e-5,momentum=0.1)
        self.fc = nn.Sequential(
            nn.Linear(1,32),
            nn.BatchNorm1d(32,eps=1e-5,momentum=0.1),
            nn.Tanh(),

            nn.Linear(32,16),
            nn.BatchNorm1d(16, eps=1e-5, momentum=0.1),
            nn.Tanh(),

            nn.Linear(16,8),
            nn.BatchNorm1d(8, eps=1e-5, momentum=0.1),
            nn.Tanh(),

            nn.Linear(8,4),
            nn.BatchNorm1d(4, eps=1e-5, momentum=0.1),
            nn.Tanh(),
        )
        self.out = nn.Linear(4,1)

    def forward(self, x):
        x = self.norm(x)
        x = self.fc(x)
        x = self.out(x)
        return x

net = Net()

optimizer = torch.optim.Adam(net.parameters(),lr=0.3)
loss_func = nn.MSELoss()

if __name__ == '__main__':
    for epoch in range(EPOCH):
        for step,(x,y) in enumerate(train_loader):
            b_x = Variable(x)
            b_y = Variable(y)
            prediction = net(b_x)
            loss = loss_func(prediction,b_y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if step%50 == 0 :
                 print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())
